{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:14:54.494148Z",
     "start_time": "2021-11-11T22:14:47.643952Z"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import os\n",
    "import unidecode\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import*\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "## 2. Importar base SS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importar base SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:17:41.758469Z",
     "start_time": "2021-11-11T22:17:41.126860Z"
    }
   },
   "outputs": [],
   "source": [
    "# create an empty pandas data frame\n",
    "df = pd.DataFrame()\n",
    " \n",
    "# iterate over all files within \"My_Folder\"\n",
    "for file in os.listdir(\"social_studio\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.concat([df , pd.read_csv(os.path.join(\"social_studio\", file))], axis=0 )\n",
    " \n",
    "# reset the index \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.drop_duplicates(keep='first')\n",
    "df=df.drop(df.columns[[0, 1]], axis=1) \n",
    "df.to_csv('ss_completo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:17:41.774081Z",
     "start_time": "2021-11-11T22:17:41.760171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17983, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procesamiento base ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:17:41.946003Z",
     "start_time": "2021-11-11T22:17:41.933998Z"
    }
   },
   "outputs": [],
   "source": [
    "tablacomments=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:17:44.282987Z",
     "start_time": "2021-11-11T22:17:42.190541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convierte a tipo fecha la columna PUBLISH_DATE\n",
    "tablacomments[\"PUBLISH_DATE\"]=pd.to_datetime(tablacomments[\"PUBLISH_DATE\"])\n",
    "# Selecciona columnas de interés, filtra el contenido, da formato a la decha y pone en minúsculas la columna 'CONTENT'. Finalmente imprime la base de datos:\n",
    "TABLACONTENT= tablacomments[[\"CONTENT\", \"REGION\", \"FOLLOWERS\", \"PUBLISH_DATE\", \"AUTHOR\",'ARTICLE_URL']]\n",
    "TABLACONTENT= TABLACONTENT[(~TABLACONTENT[\"CONTENT\"].isna())&(TABLACONTENT[\"CONTENT\"]!=\"Content not available\" )]\n",
    "TABLACONTENT[\"FECHA\"]=TABLACONTENT[\"PUBLISH_DATE\"].dt.strftime(\"%m/%d/%Y\")\n",
    "TABLACONTENT['CONTENT'] = TABLACONTENT['CONTENT'].str.lower()\n",
    "\n",
    "#Normalizar contenido\n",
    "TABLACONTENT['CONTENT_']=TABLACONTENT['CONTENT'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8').copy()\n",
    "#TABLACONTENT['PUBLISH_DATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtro fecha\n",
    "TABLACONTENT_Marzo = TABLACONTENT[TABLACONTENT[\"PUBLISH_DATE\"] >='2021-03-01T04:36:24.000000000']\n",
    "TABLACONTENT = TABLACONTENT_Marzo\n",
    "#TABLACONTENT = TABLACONTENT.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "TABLACONTENT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:17:48.763758Z",
     "start_time": "2021-11-11T22:17:48.734836Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################### MODIFICAR (opcional) ####################\n",
    "palabra = 'dob'\n",
    "prueba = TABLACONTENT.loc[TABLACONTENT['CONTENT'].str.contains(palabra),['CONTENT','AUTHOR','FECHA']]#\n",
    "prueba.CONTENT.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:18:13.467914Z",
     "start_time": "2021-11-11T22:18:13.436997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quita los textos que contienen estas palabras (usar con cuidado)\n",
    "\n",
    "PALABRAS_FILTRO = [\n",
    "                  ]\n",
    "\n",
    "TABLACONTENT = TABLACONTENT[~TABLACONTENT.CONTENT.apply(lambda x: any(y in x for y in PALABRAS_FILTRO))]\n",
    "TABLACONTENT.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtro por procesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Consumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 4.1.1. Wordcloud general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:31:52.123660Z",
     "start_time": "2021-11-11T22:31:51.809704Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Contar cant de tweets asociados al filtro\n",
    "\n",
    "PALABRAS_FILTRO = [ 'lectura', 'consumo', 'doble', 'errada', 'regist'\n",
    "                  ]\n",
    "\n",
    "df_consumo = TABLACONTENT[TABLACONTENT.CONTENT_.apply(lambda x: any(y in x for y in PALABRAS_FILTRO))]\n",
    "#df_consumo=df_consumo.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "\n",
    "\n",
    "#filtro por proceso\n",
    "searchfor=['consumo', 'lectura', 'errada', 'doble', 'regist' ]\n",
    "\n",
    "texto_consumo=''.join(df_consumo[df_consumo.CONTENT_.str.contains('|'.join(searchfor))].fillna('')['CONTENT_'].tolist())\n",
    "df_consumo.CONTENT.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:31:58.827546Z",
     "start_time": "2021-11-11T22:31:58.800618Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PALABRAS_FILTRO = [ 'dobl'\n",
    "                  ]\n",
    "\n",
    "df_suspencion_= df_consumo[df_consumo.CONTENT_.apply(lambda x: any(y in x for y in PALABRAS_FILTRO))]\n",
    "#df_RPO=df_RPO.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "\n",
    "\n",
    "df_suspencion_['CONTENT'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:31:13.086847Z",
     "start_time": "2021-11-11T22:30:26.739873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#lematizar\n",
    "text = texto_consumo\n",
    "doc = nlp(text)\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:09:31.813722Z",
     "start_time": "2021-11-05T02:09:31.784414Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pasar de lista a str\n",
    "texto_consumo_ = ' '.join([str(elem) for elem in lemmas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:09:31.844644Z",
     "start_time": "2021-11-05T02:09:31.821862Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Definir stopwords\n",
    "from stop_words import get_stop_words\n",
    "stop_words=get_stop_words('spanish')\n",
    "stop_words.append('HTTP')\n",
    "stop_words.append('HTTPS')\n",
    "stop_words.append('t')\n",
    "stop_words.append('ga')\n",
    "stop_words.append('hacer')\n",
    "stop_words.append('tener')\n",
    "stop_words.append('ser')\n",
    "stop_words.append('haber')\n",
    "stop_words.append('poder')\n",
    "\n",
    "sw=set(STOPWORDS)\n",
    "sw=stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:09:38.141175Z",
     "start_time": "2021-11-05T02:09:31.847546Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wc=WordCloud(collocations=True,\n",
    "             collocation_threshold=10,\n",
    "            stopwords=sw,\n",
    "              background_color='white',\n",
    "              width=1500,\n",
    "              height=800,\n",
    "              max_words=80,\n",
    "             colormap='Dark2',\n",
    "                    ).generate(texto_consumo_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,20), sharex=True, sharey=True)\n",
    "\n",
    "plt.gca().imshow(wc)\n",
    "plt.gca().set_title('Topic Global', fontdict=dict(size=16))\n",
    "plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('raspones_Wc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 4.1.2. Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:09:46.003128Z",
     "start_time": "2021-11-05T02:09:38.143172Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#bigramas\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:09:46.018195Z",
     "start_time": "2021-11-05T02:09:46.005088Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texto_consumo_big=df_consumo['CONTENT_'].unique().tolist()\n",
    "texto_consumo_big[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:09:46.033656Z",
     "start_time": "2021-11-05T02:09:46.022641Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Definir stopwords\n",
    "from stop_words import get_stop_words\n",
    "stop_words=get_stop_words('spanish')\n",
    "stop_words.append('HTTP')\n",
    "stop_words.append('https')\n",
    "stop_words.append('HTTPS')\n",
    "stop_words.append('CO')\n",
    "stop_words.append('@')\n",
    "stop_words.append('hacer')\n",
    "stop_words.append('tener')\n",
    "stop_words.append('ser')\n",
    "stop_words.append('haber')\n",
    "stop_words.append('poder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:09:46.064531Z",
     "start_time": "2021-11-05T02:09:46.040538Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    #return stemmer.stem(text)\n",
    "    doc = nlp(text)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    texto_vanti_ = ' '.join([str(elem) for elem in lemmas])\n",
    "    return texto_vanti_\n",
    "\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in stop_words and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "           \n",
    "            #result.append((token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:12:10.270189Z",
     "start_time": "2021-11-05T02:09:46.070459Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in texto_consumo_big:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:12:10.315843Z",
     "start_time": "2021-11-05T02:12:10.275039Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:12:10.471582Z",
     "start_time": "2021-11-05T02:12:10.322019Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create list of lists containing bigrams in tweets\n",
    "terms_bigram = [list(bigrams(tweet)) for tweet in processed_docs]\n",
    "\n",
    "# View bigrams for the first tweet\n",
    "terms_bigram[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:12:10.518790Z",
     "start_time": "2021-11-05T02:12:10.474578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Flatten list of bigrams in clean tweets\n",
    "bigrams = list(itertools.chain(*terms_bigram))\n",
    "\n",
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = collections.Counter(bigrams)\n",
    "\n",
    "bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:12:10.548814Z",
     "start_time": "2021-11-05T02:12:10.524506Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_df = pd.DataFrame(bigram_counts.most_common(15),\n",
    "                             columns=['bigram', 'count'])\n",
    "bigram_df['bigram']=bigram_df['bigram'].astype(str)\n",
    "bigram=list((bigram_df['bigram']))\n",
    "count=list((bigram_df['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:12:11.104061Z",
     "start_time": "2021-11-05T02:12:10.554798Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = bigram_df.iloc[::-1]\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.barh(bigram, count, align='center', alpha=0.5, color='purple')\n",
    "#plt.yticks(y_pos, )\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.title('Frecuencia de bigramas para consumo')\n",
    "#plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 4.1.3 n-grams, principales narrativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:18:52.085482Z",
     "start_time": "2021-11-05T02:18:50.759507Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "df = df_consumo.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "nlp = spacy.load('es_core_news_sm') # español \n",
    "def normalize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    lemmas=\" \".join(lemmas)\n",
    "    doc1 = nlp(lemmas)\n",
    "    words = [t.orth_ for t in doc1 if not t.is_punct ]\n",
    "    lexical_tokens = [t.lower() for t in words if len(t) > 3 and t.isalpha()]\n",
    "    return lexical_tokens\n",
    "\n",
    "\n",
    "# list contains  punctuation\n",
    "sw_list = stopwords.words('spanish')\n",
    "sw_list.append('oficiales')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list = []\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘',\"'\", '©',\n",
    "'@', '-', '–','--','—', '_','!!','¡¡','¿','?','!','¡','.co','http ','#','http',':','//']\n",
    "sw_set = set(sw_list)\n",
    "\n",
    "# tokenization\n",
    "def process_data(string):\n",
    "    tokens = nltk.word_tokenize(string) # tokenization\n",
    "    punctuation_removed = [token.lower() for token in tokens if token.lower() not in sw_set]\n",
    "    punctuation_removed = [token.lower() for token in tokens ]\n",
    "    return punctuation_removed\n",
    "\n",
    "# create a function stemming() and loop through each word in a review\n",
    "def stemming(string):\n",
    "    # Stemming\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_string=[]\n",
    "    for w in string:\n",
    "        stemmed_string.append(ps.stem(w))\n",
    "    return stemmed_string\n",
    "\n",
    "\n",
    "# create a function  and loop through each word in  a review\n",
    "def lemmatization(string):\n",
    "    # import libraries\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list=[]\n",
    "    for word in string:\n",
    "        lemma_word=lemmatizer.lemmatize(word,pos='v') \n",
    "        lemma_list.append(lemma_word)\n",
    "    return lemma_list\n",
    "\n",
    "# Conbime all functions above and obtian cleaned text data \n",
    "def data_preprocessing(text_data):\n",
    "    ##tokenization, stop words removal, punctuation marks removel\n",
    "    processed_string=list(map(process_data,text_data))\n",
    "    #stemming\n",
    "    # stemming_string=list(map(stemming,processed_string)) #Data out prurals \n",
    "    ## lemmatization\n",
    "    lemma_string=list(map(lemmatization,processed_string))\n",
    "    \n",
    "    return lemma_string\n",
    "\n",
    "cleaned_data = data_preprocessing(df['CONTENT'])\n",
    "merged_data=[]\n",
    "for i in range(len(cleaned_data )):\n",
    "    merged_data.append(\" \".join(cleaned_data [i]))\n",
    "merged_df=pd.DataFrame(merged_data)\n",
    "merged_df.to_csv(\"merged_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:24:45.870280Z",
     "start_time": "2021-11-05T02:24:44.978006Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# obtener los ngrams  \n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vec = CountVectorizer(ngram_range=(4, 6)).fit(corpus) # minimo y maximo de palabras \n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def bigram_dictionary(data,n=None):\n",
    "    # n es la cantidad de n_grams que se desean adquirir \n",
    "    words_freq=get_top_n_bigram(data,n)\n",
    "    \n",
    "    dictionary={}\n",
    "    for i in range(len(words_freq)):\n",
    "        dictionary[words_freq[i][0]]=words_freq[i][1]\n",
    "    return dictionary\n",
    "\n",
    "BOW = bigram_dictionary(merged_data,510)# get top (merged_data,n= )\n",
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:24:44.329724Z",
     "start_time": "2021-11-05T02:24:28.437961Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Nos quedamos con el BAG que tengan un umbral de similitud de 0.3 (no se parezcan ) \n",
    "# y con la frase más larga  \n",
    "sentences = []\n",
    "BOW_new = {}\n",
    "for key0,value0 in BOW.items():\n",
    "    same = [ key for key,value in BOW.items() if value0==value]\n",
    "    max_same = max(same)\n",
    "    same_similarity = [s for s in same if nlp(s).similarity(nlp(max_same))<=0.3] + [max_same] #0.45 Umbral de similitud \n",
    "    sentences.append(same_similarity)\n",
    "    frases = []\n",
    "    for s in range(len(sentences)):\n",
    "        lista = list(map(len,sentences[s]))\n",
    "        try:\n",
    "            maxi = max(lista)\n",
    "            frases.append(sentences[s][lista.index(maxi)])\n",
    "            frases.append(sentences[s][-1])\n",
    "        except:\n",
    "            continue \n",
    "            \n",
    "for i in list(dict.fromkeys(frases)):\n",
    "    BOW_new[i] = BOW[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T02:19:09.360877Z",
     "start_time": "2021-11-05T02:18:54.055922Z"
    },
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# twitter_mask = np.array(Image.open('./twitter_mask.png'))\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=1500,\n",
    "                  height=800,\n",
    "                  colormap='tab10',\n",
    "                  max_words=50,\n",
    "                  prefer_horizontal=1.0\n",
    "                 )\n",
    "                  \n",
    "plt.figure(figsize=(10,10), facecolor='w')\n",
    "Wcloud = cloud.generate_from_frequencies(BOW_new, max_font_size=800)\n",
    "plt.gca().imshow(Wcloud)\n",
    "plt.gca().set_title('Principales narrativas', fontdict=dict(size=16))\n",
    "plt.gca().axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 4.2. RPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.2.1. Wordcloud general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:04:21.343720Z",
     "start_time": "2021-11-05T15:04:21.256949Z"
    },
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Contar cant de tweets asociados al filtro\n",
    "\n",
    "PALABRAS_FILTRO = [ 'RPO', 'obligatoria','revisión','revision', 'anual'\n",
    "                  ]\n",
    "\n",
    "df_RPO = TABLACONTENT[TABLACONTENT.CONTENT_.apply(lambda x: any(y in x for y in PALABRAS_FILTRO))]\n",
    "#df_RPO=df_RPO.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "\n",
    "#filtro por proceso\n",
    "searchfor=['RPO', 'obligatoria', 'revisión','revision', 'anual']\n",
    "\n",
    "texto_RPO=''.join(df_RPO[df_RPO.CONTENT_.str.contains('|'.join(searchfor))].fillna('')['CONTENT_'].tolist())\n",
    "df_RPO['CONTENT'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:04:21.374637Z",
     "start_time": "2021-11-05T15:04:21.349701Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PALABRAS_FILTRO = [ 'el tecnico', 'operario','operador','visita',\n",
    "                  ]\n",
    "\n",
    "df_RPO = df_RPO[df_RPO.CONTENT_.apply(lambda x: any(y in x for y in PALABRAS_FILTRO))]\n",
    "#df_RPO=df_RPO.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "\n",
    "texto_RPO=''.join(df_RPO[df_RPO.CONTENT_.str.contains('|'.join(searchfor))].fillna('')['CONTENT_'].tolist())\n",
    "df_RPO['CONTENT'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:06:46.476410Z",
     "start_time": "2021-11-05T15:06:46.467434Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PALABRAS_FILTRO = [ 'etica']\n",
    "\n",
    "df_RPO_filtr = df_RPO[df_RPO.CONTENT_.apply(lambda x: any(y in x for y in PALABRAS_FILTRO))]\n",
    "df_RPO_filtr.CONTENT_.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:04:21.421510Z",
     "start_time": "2021-11-05T15:04:21.411539Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "498/1319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:04:23.561784Z",
     "start_time": "2021-11-05T15:04:21.486336Z"
    },
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#lematizar\n",
    "text = texto_RPO\n",
    "doc = nlp(text)\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:04:23.577740Z",
     "start_time": "2021-11-05T15:04:23.563779Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pasar de lista a str\n",
    "texto_RPO_ = ' '.join([str(elem) for elem in lemmas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:04:23.609657Z",
     "start_time": "2021-11-05T15:04:23.585719Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Definir stopwords\n",
    "from stop_words import get_stop_words\n",
    "stop_words=get_stop_words('spanish')\n",
    "stop_words.append('HTTP')\n",
    "stop_words.append('HTTPS')\n",
    "stop_words.append('t')\n",
    "stop_words.append('si')\n",
    "stop_words.append('hacer')\n",
    "stop_words.append('tener')\n",
    "stop_words.append('ser')\n",
    "stop_words.append('ma')\n",
    "stop_words.append('poder')\n",
    "\n",
    "sw=set(STOPWORDS)\n",
    "sw=stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:04:30.847020Z",
     "start_time": "2021-11-05T15:04:23.612649Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wc=WordCloud(collocations=True,\n",
    "             collocation_threshold=10,\n",
    "            stopwords=sw,\n",
    "              background_color='white',\n",
    "              width=1500,\n",
    "              height=800,\n",
    "              max_words=80,\n",
    "             colormap='Dark2',\n",
    "                    ).generate(texto_RPO_)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,20), sharex=True, sharey=True)\n",
    "\n",
    "plt.gca().imshow(wc)\n",
    "plt.gca().set_title('Topic Global', fontdict=dict(size=16))\n",
    "plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.2.2. Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:04:54.777244Z",
     "start_time": "2021-11-05T15:04:51.822278Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#bigramas\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:04:54.809170Z",
     "start_time": "2021-11-05T15:04:54.783231Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texto_RPO_big=df_RPO['CONTENT_'].unique().tolist()\n",
    "texto_RPO_big[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:04:54.825157Z",
     "start_time": "2021-11-05T15:04:54.815151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Definir stopwords\n",
    "from stop_words import get_stop_words\n",
    "stop_words=get_stop_words('spanish')\n",
    "stop_words.append('HTTP')\n",
    "stop_words.append('https')\n",
    "stop_words.append('HTTPS')\n",
    "stop_words.append('CO')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:04:54.840082Z",
     "start_time": "2021-11-05T15:04:54.830111Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    #return stemmer.stem(text)\n",
    "    doc = nlp(text)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    texto_vanti_ = ' '.join([str(elem) for elem in lemmas])\n",
    "    return texto_vanti_\n",
    "\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in stop_words and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "           \n",
    "            #result.append((token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:05:11.109557Z",
     "start_time": "2021-11-05T15:04:54.843073Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in texto_RPO_big:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:05:11.125516Z",
     "start_time": "2021-11-05T15:05:11.111552Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:05:11.157488Z",
     "start_time": "2021-11-05T15:05:11.130504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create list of lists containing bigrams in tweets\n",
    "terms_bigram_ = [list(bigrams(tweet)) for tweet in processed_docs]\n",
    "\n",
    "# View bigrams for the first tweet\n",
    "terms_bigram_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:05:11.173397Z",
     "start_time": "2021-11-05T15:05:11.160422Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Flatten list of bigrams in clean tweets\n",
    "bigrams = list(itertools.chain(*terms_bigram_))\n",
    "\n",
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = collections.Counter(bigrams)\n",
    "\n",
    "bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:05:16.012552Z",
     "start_time": "2021-11-05T15:05:16.000545Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_df = pd.DataFrame(bigram_counts.most_common(15),\n",
    "                             columns=['bigram', 'count'])\n",
    "\n",
    "bigram_df['bigram']=bigram_df['bigram'].astype(str)\n",
    "bigram=list((bigram_df['bigram']))\n",
    "count=list((bigram_df['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T15:05:16.813525Z",
     "start_time": "2021-11-05T15:05:16.233675Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = bigram_df.iloc[::-1]\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.barh(bigram, count, align='center', alpha=0.5, color='purple')\n",
    "#plt.yticks(y_pos, )\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.title('Frecuencia de bigramas para RPO')\n",
    "#plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.2.3. n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T14:54:57.222760Z",
     "start_time": "2021-11-05T14:54:52.483549Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "df = df_RPO.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "nlp = spacy.load('es_core_news_sm') # español \n",
    "def normalize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    lemmas=\" \".join(lemmas)\n",
    "    doc1 = nlp(lemmas)\n",
    "    words = [t.orth_ for t in doc1 if not t.is_punct ]\n",
    "    lexical_tokens = [t.lower() for t in words if len(t) > 3 and t.isalpha()]\n",
    "    return lexical_tokens\n",
    "\n",
    "\n",
    "# list contains  punctuation\n",
    "sw_list = stopwords.words('spanish')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list = []\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘',\"'\", '©',\n",
    "'@', '-', '–','--','—', '_','!!','¡¡','¿','?','!','¡','.co','http ','#','http',':','//']\n",
    "sw_set = set(sw_list)\n",
    "\n",
    "# tokenization\n",
    "def process_data(string):\n",
    "    tokens = nltk.word_tokenize(string) # tokenization\n",
    "    punctuation_removed = [token.lower() for token in tokens if token.lower() not in sw_set]\n",
    "    punctuation_removed = [token.lower() for token in tokens ]\n",
    "    return punctuation_removed\n",
    "\n",
    "# create a function stemming() and loop through each word in a review\n",
    "def stemming(string):\n",
    "    # Stemming\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_string=[]\n",
    "    for w in string:\n",
    "        stemmed_string.append(ps.stem(w))\n",
    "    return stemmed_string\n",
    "\n",
    "\n",
    "# create a function  and loop through each word in  a review\n",
    "def lemmatization(string):\n",
    "    # import libraries\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list=[]\n",
    "    for word in string:\n",
    "        lemma_word=lemmatizer.lemmatize(word,pos='v') \n",
    "        lemma_list.append(lemma_word)\n",
    "    return lemma_list\n",
    "\n",
    "# Conbime all functions above and obtian cleaned text data \n",
    "def data_preprocessing(text_data):\n",
    "    ##tokenization, stop words removal, punctuation marks removel\n",
    "    processed_string=list(map(process_data,text_data))\n",
    "    #stemming\n",
    "    # stemming_string=list(map(stemming,processed_string)) #Data out prurals \n",
    "    ## lemmatization\n",
    "    lemma_string=list(map(lemmatization,processed_string))\n",
    "    \n",
    "    return lemma_string\n",
    "\n",
    "cleaned_data = data_preprocessing(df['CONTENT'])\n",
    "merged_data=[]\n",
    "for i in range(len(cleaned_data )):\n",
    "    merged_data.append(\" \".join(cleaned_data [i]))\n",
    "merged_df=pd.DataFrame(merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T14:54:57.614173Z",
     "start_time": "2021-11-05T14:54:57.225279Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# obtener los ngrams  \n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vec = CountVectorizer(ngram_range=(4, 6)).fit(corpus) # minimo y maximo de palabras \n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def bigram_dictionary(data,n=None):\n",
    "    # n es la cantidad de n_grams que se desean adquirir \n",
    "    words_freq=get_top_n_bigram(data,n)\n",
    "    \n",
    "    dictionary={}\n",
    "    for i in range(len(words_freq)):\n",
    "        dictionary[words_freq[i][0]]=words_freq[i][1]\n",
    "    return dictionary\n",
    "\n",
    "BOW = bigram_dictionary(merged_data,15)# get top (merged_data,n= )\n",
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T14:54:58.627911Z",
     "start_time": "2021-11-05T14:54:57.616113Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Nos quedamos con el BAG que tengan un umbral de similitud de 0.3 (no se parezcan ) \n",
    "# y con la frase más larga  \n",
    "sentences = []\n",
    "BOW_new = {}\n",
    "for key0,value0 in BOW.items():\n",
    "    same = [ key for key,value in BOW.items() if value0==value]\n",
    "    max_same = max(same)\n",
    "    same_similarity = [s for s in same if nlp(s).similarity(nlp(max_same))<=0.3] + [max_same] #0.45 Umbral de similitud \n",
    "    sentences.append(same_similarity)\n",
    "    frases = []\n",
    "    for s in range(len(sentences)):\n",
    "        lista = list(map(len,sentences[s]))\n",
    "        try:\n",
    "            maxi = max(lista)\n",
    "            frases.append(sentences[s][lista.index(maxi)])\n",
    "            frases.append(sentences[s][-1])\n",
    "        except:\n",
    "            continue \n",
    "            \n",
    "for i in list(dict.fromkeys(frases)):\n",
    "    BOW_new[i] = BOW[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T14:55:03.623108Z",
     "start_time": "2021-11-05T14:54:58.631905Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# twitter_mask = np.array(Image.open('./twitter_mask.png'))\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=1500,\n",
    "                  height=800,\n",
    "                  colormap='tab10',\n",
    "                  max_words=50,\n",
    "                  prefer_horizontal=1.0\n",
    "                 )\n",
    "                  \n",
    "plt.figure(figsize=(10,10), facecolor='w')\n",
    "Wcloud = cloud.generate_from_frequencies(BOW_new, max_font_size=800)\n",
    "plt.gca().imshow(Wcloud)\n",
    "plt.gca().set_title('Principales narrativas', fontdict=dict(size=16))\n",
    "plt.gca().axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 4.3. Suspención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.3.1. Wordcloud general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:27:57.499447Z",
     "start_time": "2021-11-11T22:27:57.446588Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Contar cant de tweets asociados al filtro\n",
    "\n",
    "PALABRAS_FILTRO = [ 'susp', 'corta', 'fecha', 'limite'\n",
    "                  ]\n",
    "\n",
    "df_suspencion = TABLACONTENT[TABLACONTENT.CONTENT_.apply(lambda x: any(y in x for y in PALABRAS_FILTRO))]\n",
    "#df_consumo=df_consumo.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "\n",
    "\n",
    "#filtro por proceso\n",
    "searchfor=['susp' , 'corta', 'fecha', 'limite']\n",
    "\n",
    "texto_suspencion=''.join(df_suspencion[df_suspencion.CONTENT_.str.contains('|'.join(searchfor))].fillna('')['CONTENT_'].tolist())\n",
    "df_suspencion.CONTENT_.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T22:28:18.271938Z",
     "start_time": "2021-11-11T22:28:18.258974Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PALABRAS_FILTRO = [ 'doble'\n",
    "                  ]\n",
    "\n",
    "df_suspencion_= df_suspencion[df_suspencion.CONTENT_.apply(lambda x: any(y in x for y in PALABRAS_FILTRO))]\n",
    "#df_RPO=df_RPO.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "\n",
    "\n",
    "df_suspencion_['CONTENT'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:46:47.833067Z",
     "start_time": "2021-11-11T01:46:06.772704Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#lematizar\n",
    "text = texto_suspencion\n",
    "doc = nlp(text)\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:46:47.884896Z",
     "start_time": "2021-11-11T01:46:47.838432Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pasar de lista a str\n",
    "texto_suspencion_ = ' '.join([str(elem) for elem in lemmas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:46:47.922283Z",
     "start_time": "2021-11-11T01:46:47.892615Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Definir stopwords\n",
    "from stop_words import get_stop_words\n",
    "stop_words=get_stop_words('spanish')\n",
    "stop_words.append('HTTP')\n",
    "stop_words.append('HTTPS')\n",
    "stop_words.append('t')\n",
    "stop_words.append('ga')\n",
    "stop_words.append('hacer')\n",
    "stop_words.append('tener')\n",
    "stop_words.append('ser')\n",
    "stop_words.append('haber')\n",
    "stop_words.append('poder')\n",
    "stop_words.append('co')\n",
    "stop_words.append('decir')\n",
    "\n",
    "sw=set(STOPWORDS)\n",
    "sw=stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:46:54.629120Z",
     "start_time": "2021-11-11T01:46:47.927453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wc=WordCloud(collocations=True,\n",
    "             collocation_threshold=30,\n",
    "            stopwords=sw,\n",
    "              background_color='white',\n",
    "              width=1500,\n",
    "              height=800,\n",
    "              max_words=80,\n",
    "             colormap='Dark2',\n",
    "                    ).generate(texto_suspencion_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,20), sharex=True, sharey=True)\n",
    "\n",
    "plt.gca().imshow(wc)\n",
    "plt.gca().set_title('Topic Global', fontdict=dict(size=16))\n",
    "plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.3.2. Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:47:02.681099Z",
     "start_time": "2021-11-11T01:46:54.632518Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#bigramas\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:47:02.707257Z",
     "start_time": "2021-11-11T01:47:02.689304Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texto_suspencion_big=df_suspencion['CONTENT_'].unique().tolist()\n",
    "texto_suspencion_big[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:47:02.737890Z",
     "start_time": "2021-11-11T01:47:02.712739Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Definir stopwords\n",
    "from stop_words import get_stop_words\n",
    "stop_words=get_stop_words('spanish')\n",
    "stop_words.append('HTTP')\n",
    "stop_words.append('https')\n",
    "stop_words.append('HTTPS')\n",
    "stop_words.append('CO')\n",
    "stop_words.append('@')\n",
    "stop_words.append('hacer')\n",
    "stop_words.append('tener')\n",
    "stop_words.append('ser')\n",
    "stop_words.append('haber')\n",
    "stop_words.append('natural')\n",
    "stop_words.append('servicio')\n",
    "stop_words.append('gas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:47:02.762892Z",
     "start_time": "2021-11-11T01:47:02.745875Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    #return stemmer.stem(text)\n",
    "    doc = nlp(text)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    texto_vanti_ = ' '.join([str(elem) for elem in lemmas])\n",
    "    return texto_vanti_\n",
    "\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in stop_words and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "           \n",
    "            #result.append((token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:49:05.158926Z",
     "start_time": "2021-11-11T01:47:02.767959Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in texto_suspencion_big:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:49:05.201775Z",
     "start_time": "2021-11-11T01:49:05.165357Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:49:05.271051Z",
     "start_time": "2021-11-11T01:49:05.212076Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create list of lists containing bigrams in tweets\n",
    "terms_bigram = [list(bigrams(tweet)) for tweet in processed_docs]\n",
    "\n",
    "# View bigrams for the first tweet\n",
    "terms_bigram[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:49:05.356786Z",
     "start_time": "2021-11-11T01:49:05.283604Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Flatten list of bigrams in clean tweets\n",
    "bigrams = list(itertools.chain(*terms_bigram))\n",
    "\n",
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = collections.Counter(bigrams)\n",
    "\n",
    "bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:49:05.390029Z",
     "start_time": "2021-11-11T01:49:05.363460Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_df = pd.DataFrame(bigram_counts.most_common(15),\n",
    "                             columns=['bigram', 'count'])\n",
    "bigram_df['bigram']=bigram_df['bigram'].astype(str)\n",
    "bigram=list((bigram_df['bigram']))\n",
    "count=list((bigram_df['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T01:49:06.016492Z",
     "start_time": "2021-11-11T01:49:05.393687Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = bigram_df.iloc[::-1]\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.barh(bigram, count, align='center', alpha=0.5, color='purple')\n",
    "#plt.yticks(y_pos, )\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.title('Frecuencia de bigramas para suspencion')\n",
    "#plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.3.3 n-grams, principales narrativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T12:42:34.057773Z",
     "start_time": "2021-11-09T12:42:32.264423Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "df = df_suspencion.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "nlp = spacy.load('es_core_news_sm') # español \n",
    "def normalize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    lemmas=\" \".join(lemmas)\n",
    "    doc1 = nlp(lemmas)\n",
    "    words = [t.orth_ for t in doc1 if not t.is_punct ]\n",
    "    lexical_tokens = [t.lower() for t in words if len(t) > 3 and t.isalpha()]\n",
    "    return lexical_tokens\n",
    "\n",
    "\n",
    "# list contains  punctuation\n",
    "sw_list = stopwords.words('spanish')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list = []\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘',\"'\", '©',\n",
    "'@', '-', '–','--','—', '_','!!','¡¡','¿','?','!','¡','.co','http ','#','http',':','//']\n",
    "sw_set = set(sw_list)\n",
    "\n",
    "# tokenization\n",
    "def process_data(string):\n",
    "    tokens = nltk.word_tokenize(string) # tokenization\n",
    "    punctuation_removed = [token.lower() for token in tokens if token.lower() not in sw_set]\n",
    "    punctuation_removed = [token.lower() for token in tokens ]\n",
    "    return punctuation_removed\n",
    "\n",
    "# create a function stemming() and loop through each word in a review\n",
    "def stemming(string):\n",
    "    # Stemming\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_string=[]\n",
    "    for w in string:\n",
    "        stemmed_string.append(ps.stem(w))\n",
    "    return stemmed_string\n",
    "\n",
    "\n",
    "# create a function  and loop through each word in  a review\n",
    "def lemmatization(string):\n",
    "    # import libraries\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list=[]\n",
    "    for word in string:\n",
    "        lemma_word=lemmatizer.lemmatize(word,pos='v') \n",
    "        lemma_list.append(lemma_word)\n",
    "    return lemma_list\n",
    "\n",
    "# Conbime all functions above and obtian cleaned text data \n",
    "def data_preprocessing(text_data):\n",
    "    ##tokenization, stop words removal, punctuation marks removel\n",
    "    processed_string=list(map(process_data,text_data))\n",
    "    #stemming\n",
    "    # stemming_string=list(map(stemming,processed_string)) #Data out prurals \n",
    "    ## lemmatization\n",
    "    lemma_string=list(map(lemmatization,processed_string))\n",
    "    \n",
    "    return lemma_string\n",
    "\n",
    "cleaned_data = data_preprocessing(df['CONTENT'])\n",
    "merged_data=[]\n",
    "for i in range(len(cleaned_data )):\n",
    "    merged_data.append(\" \".join(cleaned_data [i]))\n",
    "merged_df=pd.DataFrame(merged_data)\n",
    "#merged_df.to_csv(\"merged_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T12:42:34.744992Z",
     "start_time": "2021-11-09T12:42:34.061169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# obtener los ngrams  \n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vec = CountVectorizer(ngram_range=(4, 6)).fit(corpus) # minimo y maximo de palabras \n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def bigram_dictionary(data,n=None):\n",
    "    # n es la cantidad de n_grams que se desean adquirir \n",
    "    words_freq=get_top_n_bigram(data,n)\n",
    "    \n",
    "    dictionary={}\n",
    "    for i in range(len(words_freq)):\n",
    "        dictionary[words_freq[i][0]]=words_freq[i][1]\n",
    "    return dictionary\n",
    "\n",
    "BOW = bigram_dictionary(merged_data,15)# get top (merged_data,n= )\n",
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T12:42:35.153611Z",
     "start_time": "2021-11-09T12:42:34.749980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Nos quedamos con el BAG que tengan un umbral de similitud de 0.3 (no se parezcan ) \n",
    "# y con la frase más larga  \n",
    "sentences = []\n",
    "BOW_new = {}\n",
    "for key0,value0 in BOW.items():\n",
    "    same = [ key for key,value in BOW.items() if value0==value]\n",
    "    max_same = max(same)\n",
    "    same_similarity = [s for s in same if nlp(s).similarity(nlp(max_same))<=0.3] + [max_same] #0.45 Umbral de similitud \n",
    "    sentences.append(same_similarity)\n",
    "    frases = []\n",
    "    for s in range(len(sentences)):\n",
    "        lista = list(map(len,sentences[s]))\n",
    "        try:\n",
    "            maxi = max(lista)\n",
    "            frases.append(sentences[s][lista.index(maxi)])\n",
    "            frases.append(sentences[s][-1])\n",
    "        except:\n",
    "            continue \n",
    "            \n",
    "for i in list(dict.fromkeys(frases)):\n",
    "    BOW_new[i] = BOW[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T12:42:41.754887Z",
     "start_time": "2021-11-09T12:42:35.156274Z"
    },
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# twitter_mask = np.array(Image.open('./twitter_mask.png'))\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=1500,\n",
    "                  height=800,\n",
    "                  colormap='tab10',\n",
    "                  max_words=50,\n",
    "                  prefer_horizontal=1.0\n",
    "                 )\n",
    "                  \n",
    "plt.figure(figsize=(10,10), facecolor='w')\n",
    "Wcloud = cloud.generate_from_frequencies(BOW_new, max_font_size=800)\n",
    "plt.gca().imshow(Wcloud)\n",
    "plt.gca().set_title('Principales narrativas', fontdict=dict(size=16))\n",
    "plt.gca().axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 4.4. Reconexión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.4.1. Wordcloud general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T01:48:35.810109Z",
     "start_time": "2021-11-02T01:48:35.761995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Contar cant de tweets asociados al filtro\n",
    "\n",
    "PALABRAS_FILTRO = [ 'recone', \n",
    "                  ]\n",
    "\n",
    "df_reconexion = TABLACONTENT[TABLACONTENT.CONTENT_.apply(lambda x: any(y in x for y in PALABRAS_FILTRO))]\n",
    "#df_consumo=df_consumo.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "\n",
    "\n",
    "#filtro por proceso\n",
    "searchfor=['recone' ]\n",
    "\n",
    "texto_reconexion=''.join(df_reconexion[df_reconexion.CONTENT_.str.contains('|'.join(searchfor))].fillna('')['CONTENT_'].tolist())\n",
    "df_reconexion.CONTENT_.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T01:38:48.986705Z",
     "start_time": "2021-11-02T01:36:39.486317Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#lematizar\n",
    "text = texto_reconexion\n",
    "doc = nlp(text)\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T01:38:49.069097Z",
     "start_time": "2021-11-02T01:38:48.992153Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pasar de lista a str\n",
    "texto_reconexion_ = ' '.join([str(elem) for elem in lemmas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:49:06.255479Z",
     "start_time": "2021-11-02T00:49:06.244509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Definir stopwords\n",
    "from stop_words import get_stop_words\n",
    "stop_words=get_stop_words('spanish')\n",
    "stop_words.append('HTTP')\n",
    "stop_words.append('HTTPS')\n",
    "stop_words.append('t')\n",
    "stop_words.append('ga')\n",
    "stop_words.append('servicio')\n",
    "stop_words.append('tener')\n",
    "stop_words.append('ser')\n",
    "stop_words.append('haber')\n",
    "stop_words.append('poder')\n",
    "\n",
    "sw=set(STOPWORDS)\n",
    "sw=stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:49:12.907131Z",
     "start_time": "2021-11-02T00:49:06.670334Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wc=WordCloud(collocations=True,\n",
    "             collocation_threshold=10,\n",
    "            stopwords=sw,\n",
    "              background_color='white',\n",
    "              width=1500,\n",
    "              height=800,\n",
    "              max_words=80,\n",
    "             colormap='Dark2',\n",
    "                    ).generate(texto_reconexion_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,20), sharex=True, sharey=True)\n",
    "\n",
    "plt.gca().imshow(wc)\n",
    "plt.gca().set_title('Topic Global', fontdict=dict(size=16))\n",
    "plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.4.2. Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:56:29.282497Z",
     "start_time": "2021-11-02T00:56:29.265546Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#bigramas\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:56:29.515968Z",
     "start_time": "2021-11-02T00:56:29.502005Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texto_reconexion_big=df_reconexion['CONTENT_'].unique().tolist()\n",
    "texto_reconexion_big[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:56:29.689333Z",
     "start_time": "2021-11-02T00:56:29.682345Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Definir stopwords\n",
    "from stop_words import get_stop_words\n",
    "stop_words=get_stop_words('spanish')\n",
    "stop_words.append('HTTP')\n",
    "stop_words.append('https')\n",
    "stop_words.append('HTTPS')\n",
    "stop_words.append('CO')\n",
    "stop_words.append('@')\n",
    "stop_words.append('hacer')\n",
    "stop_words.append('tener')\n",
    "stop_words.append('ser')\n",
    "stop_words.append('haber')\n",
    "stop_words.append('natural')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:56:29.892974Z",
     "start_time": "2021-11-02T00:56:29.877016Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    #return stemmer.stem(text)\n",
    "    doc = nlp(text)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    texto_vanti_ = ' '.join([str(elem) for elem in lemmas])\n",
    "    return texto_vanti_\n",
    "\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in stop_words and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "           \n",
    "            #result.append((token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:57:45.779848Z",
     "start_time": "2021-11-02T00:56:30.075515Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in texto_reconexion_big:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:57:45.796045Z",
     "start_time": "2021-11-02T00:57:45.782886Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:57:45.827779Z",
     "start_time": "2021-11-02T00:57:45.799799Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create list of lists containing bigrams in tweets\n",
    "terms_bigram = [list(bigrams(tweet)) for tweet in processed_docs]\n",
    "\n",
    "# View bigrams for the first tweet\n",
    "terms_bigram[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:57:45.859286Z",
     "start_time": "2021-11-02T00:57:45.831765Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Flatten list of bigrams in clean tweets\n",
    "bigrams = list(itertools.chain(*terms_bigram))\n",
    "\n",
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = collections.Counter(bigrams)\n",
    "\n",
    "bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:57:45.874210Z",
     "start_time": "2021-11-02T00:57:45.861243Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_df = pd.DataFrame(bigram_counts.most_common(15),\n",
    "                             columns=['bigram', 'count'])\n",
    "bigram_df['bigram']=bigram_df['bigram'].astype(str)\n",
    "bigram=list((bigram_df['bigram']))\n",
    "count=list((bigram_df['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T00:57:46.347388Z",
     "start_time": "2021-11-02T00:57:45.876969Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = bigram_df.iloc[::-1]\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.barh(bigram, count, align='center', alpha=0.5, color='purple')\n",
    "#plt.yticks(y_pos, )\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.title('Frecuencia de bigramas para reconexion')\n",
    "#plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.4.3 n-grams, principales narrativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T01:00:24.533353Z",
     "start_time": "2021-11-02T01:00:23.536685Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "df = df_reconexion.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "nlp = spacy.load('es_core_news_sm') # español \n",
    "def normalize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    lemmas=\" \".join(lemmas)\n",
    "    doc1 = nlp(lemmas)\n",
    "    words = [t.orth_ for t in doc1 if not t.is_punct ]\n",
    "    lexical_tokens = [t.lower() for t in words if len(t) > 3 and t.isalpha()]\n",
    "    return lexical_tokens\n",
    "\n",
    "\n",
    "# list contains  punctuation\n",
    "sw_list = stopwords.words('spanish')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list = []\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘',\"'\", '©',\n",
    "'@', '-', '–','--','—', '_','!!','¡¡','¿','?','!','¡','.co','http ','#','http',':','//']\n",
    "sw_set = set(sw_list)\n",
    "\n",
    "# tokenization\n",
    "def process_data(string):\n",
    "    tokens = nltk.word_tokenize(string) # tokenization\n",
    "    punctuation_removed = [token.lower() for token in tokens if token.lower() not in sw_set]\n",
    "    punctuation_removed = [token.lower() for token in tokens ]\n",
    "    return punctuation_removed\n",
    "\n",
    "# create a function stemming() and loop through each word in a review\n",
    "def stemming(string):\n",
    "    # Stemming\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_string=[]\n",
    "    for w in string:\n",
    "        stemmed_string.append(ps.stem(w))\n",
    "    return stemmed_string\n",
    "\n",
    "\n",
    "# create a function  and loop through each word in  a review\n",
    "def lemmatization(string):\n",
    "    # import libraries\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list=[]\n",
    "    for word in string:\n",
    "        lemma_word=lemmatizer.lemmatize(word,pos='v') \n",
    "        lemma_list.append(lemma_word)\n",
    "    return lemma_list\n",
    "\n",
    "# Conbime all functions above and obtian cleaned text data \n",
    "def data_preprocessing(text_data):\n",
    "    ##tokenization, stop words removal, punctuation marks removel\n",
    "    processed_string=list(map(process_data,text_data))\n",
    "    #stemming\n",
    "    # stemming_string=list(map(stemming,processed_string)) #Data out prurals \n",
    "    ## lemmatization\n",
    "    lemma_string=list(map(lemmatization,processed_string))\n",
    "    \n",
    "    return lemma_string\n",
    "\n",
    "cleaned_data = data_preprocessing(df['CONTENT'])\n",
    "merged_data=[]\n",
    "for i in range(len(cleaned_data )):\n",
    "    merged_data.append(\" \".join(cleaned_data [i]))\n",
    "merged_df=pd.DataFrame(merged_data)\n",
    "merged_df.to_csv(\"merged_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T01:00:24.986818Z",
     "start_time": "2021-11-02T01:00:24.535348Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# obtener los ngrams  \n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vec = CountVectorizer(ngram_range=(4, 6)).fit(corpus) # minimo y maximo de palabras \n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def bigram_dictionary(data,n=None):\n",
    "    # n es la cantidad de n_grams que se desean adquirir \n",
    "    words_freq=get_top_n_bigram(data,n)\n",
    "    \n",
    "    dictionary={}\n",
    "    for i in range(len(words_freq)):\n",
    "        dictionary[words_freq[i][0]]=words_freq[i][1]\n",
    "    return dictionary\n",
    "\n",
    "BOW = bigram_dictionary(merged_data,15)# get top (merged_data,n= )\n",
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T01:02:06.671083Z",
     "start_time": "2021-11-02T01:02:05.598118Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Nos quedamos con el BAG que tengan un umbral de similitud de 0.3 (no se parezcan ) \n",
    "# y con la frase más larga  \n",
    "sentences = []\n",
    "BOW_new = {}\n",
    "for key0,value0 in BOW.items():\n",
    "    same = [ key for key,value in BOW.items() if value0==value]\n",
    "    max_same = max(same)\n",
    "    same_similarity = [s for s in same if nlp(s).similarity(nlp(max_same))<=0.4] + [max_same] #0.45 Umbral de similitud \n",
    "    sentences.append(same_similarity)\n",
    "    frases = []\n",
    "    for s in range(len(sentences)):\n",
    "        lista = list(map(len,sentences[s]))\n",
    "        try:\n",
    "            maxi = max(lista)\n",
    "            frases.append(sentences[s][lista.index(maxi)])\n",
    "            frases.append(sentences[s][-1])\n",
    "        except:\n",
    "            continue \n",
    "            \n",
    "for i in list(dict.fromkeys(frases)):\n",
    "    BOW_new[i] = BOW[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T01:02:14.851102Z",
     "start_time": "2021-11-02T01:02:06.677068Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# twitter_mask = np.array(Image.open('./twitter_mask.png'))\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=1500,\n",
    "                  height=800,\n",
    "                  colormap='tab10',\n",
    "                  max_words=60,\n",
    "                  prefer_horizontal=1.0\n",
    "                 )\n",
    "                  \n",
    "plt.figure(figsize=(10,10), facecolor='w')\n",
    "Wcloud = cloud.generate_from_frequencies(BOW_new, max_font_size=800)\n",
    "plt.gca().imshow(Wcloud)\n",
    "plt.gca().set_title('Principales narrativas', fontdict=dict(size=16))\n",
    "plt.gca().axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 4.5. Factura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.5.1. Wordcloud general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T01:57:09.632747Z",
     "start_time": "2021-11-02T01:57:09.571981Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Contar cant de tweets asociados al filtro\n",
    "\n",
    "PALABRAS_FILTRO = [ 'factur', \n",
    "                  ]\n",
    "\n",
    "df_factura = TABLACONTENT[TABLACONTENT.CONTENT_.apply(lambda x: any(y in x for y in PALABRAS_FILTRO))]\n",
    "#df_consumo=df_consumo.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "\n",
    "\n",
    "#filtro por proceso\n",
    "searchfor=['factur' ]\n",
    "\n",
    "texto_factura=''.join(df_factura[df_factura.CONTENT_.str.contains('|'.join(searchfor))].fillna('')['CONTENT_'].tolist())\n",
    "df_factura.CONTENT_.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:00:47.974654Z",
     "start_time": "2021-11-02T01:58:55.464720Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#lematizar\n",
    "text = texto_factura\n",
    "doc = nlp(text)\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:00:48.043799Z",
     "start_time": "2021-11-02T02:00:47.978986Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pasar de lista a str\n",
    "texto_factura_ = ' '.join([str(elem) for elem in lemmas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:02:29.508392Z",
     "start_time": "2021-11-02T02:02:29.502457Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Definir stopwords\n",
    "from stop_words import get_stop_words\n",
    "stop_words=get_stop_words('spanish')\n",
    "stop_words.append('HTTP')\n",
    "stop_words.append('HTTPS')\n",
    "stop_words.append('t')\n",
    "stop_words.append('ga')\n",
    "stop_words.append('servicio')\n",
    "stop_words.append('tener')\n",
    "stop_words.append('hacer')\n",
    "stop_words.append('haber')\n",
    "stop_words.append('poder')\n",
    "stop_words.append('ma')\n",
    "\n",
    "sw=set(STOPWORDS)\n",
    "sw=stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:02:40.504733Z",
     "start_time": "2021-11-02T02:02:30.744664Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wc=WordCloud(collocations=True,\n",
    "             collocation_threshold=10,\n",
    "            stopwords=sw,\n",
    "              background_color='white',\n",
    "              width=1500,\n",
    "              height=800,\n",
    "              max_words=80,\n",
    "             colormap='Dark2',\n",
    "                    ).generate(texto_factura_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20,20), sharex=True, sharey=True)\n",
    "\n",
    "plt.gca().imshow(wc)\n",
    "plt.gca().set_title('Topic Global', fontdict=dict(size=16))\n",
    "plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.5.2. Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:05:29.707919Z",
     "start_time": "2021-11-02T02:05:18.740126Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#bigramas\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:05:42.808065Z",
     "start_time": "2021-11-02T02:05:42.784260Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texto_factura_big=df_factura['CONTENT_'].unique().tolist()\n",
    "texto_factura_big[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:05:50.435332Z",
     "start_time": "2021-11-02T02:05:50.421130Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Definir stopwords\n",
    "from stop_words import get_stop_words\n",
    "stop_words=get_stop_words('spanish')\n",
    "stop_words.append('HTTP')\n",
    "stop_words.append('https')\n",
    "stop_words.append('HTTPS')\n",
    "stop_words.append('CO')\n",
    "stop_words.append('@')\n",
    "stop_words.append('hacer')\n",
    "stop_words.append('tener')\n",
    "stop_words.append('ser')\n",
    "stop_words.append('haber')\n",
    "stop_words.append('natural')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:05:53.154818Z",
     "start_time": "2021-11-02T02:05:53.130740Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    #return stemmer.stem(text)\n",
    "    doc = nlp(text)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    texto_vanti_ = ' '.join([str(elem) for elem in lemmas])\n",
    "    return texto_vanti_\n",
    "\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in stop_words and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "           \n",
    "            #result.append((token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:11:44.652710Z",
     "start_time": "2021-11-02T02:06:02.236242Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in texto_factura_big:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:11:44.696557Z",
     "start_time": "2021-11-02T02:11:44.684080Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processed_docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:11:44.781835Z",
     "start_time": "2021-11-02T02:11:44.698587Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create list of lists containing bigrams in tweets\n",
    "terms_bigram = [list(bigrams(tweet)) for tweet in processed_docs]\n",
    "\n",
    "# View bigrams for the first tweet\n",
    "terms_bigram[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:11:44.832630Z",
     "start_time": "2021-11-02T02:11:44.786528Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Flatten list of bigrams in clean tweets\n",
    "bigrams = list(itertools.chain(*terms_bigram))\n",
    "\n",
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = collections.Counter(bigrams)\n",
    "\n",
    "bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:11:56.245356Z",
     "start_time": "2021-11-02T02:11:56.198996Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigram_df = pd.DataFrame(bigram_counts.most_common(15),\n",
    "                             columns=['bigram', 'count'])\n",
    "bigram_df['bigram']=bigram_df['bigram'].astype(str)\n",
    "bigram=list((bigram_df['bigram']))\n",
    "count=list((bigram_df['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:13:51.981373Z",
     "start_time": "2021-11-02T02:13:50.695125Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = bigram_df.iloc[::-1]\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.barh(bigram, count, align='center', alpha=0.5, color='purple')\n",
    "#plt.yticks(y_pos, )\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.title('Frecuencia de bigramas para factura')\n",
    "#plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4.5.3 n-grams, principales narrativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:14:41.492659Z",
     "start_time": "2021-11-02T02:14:36.760847Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "df = df_factura\n",
    "#.drop_duplicates(subset='CONTENT_', keep='first')\n",
    "nlp = spacy.load('es_core_news_sm') # español \n",
    "def normalize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "    lemmas=\" \".join(lemmas)\n",
    "    doc1 = nlp(lemmas)\n",
    "    words = [t.orth_ for t in doc1 if not t.is_punct ]\n",
    "    lexical_tokens = [t.lower() for t in words if len(t) > 3 and t.isalpha()]\n",
    "    return lexical_tokens\n",
    "\n",
    "\n",
    "# list contains  punctuation\n",
    "sw_list = stopwords.words('spanish')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list = []\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘',\"'\", '©',\n",
    "'@', '-', '–','--','—', '_','!!','¡¡','¿','?','!','¡','.co','http ','#','http',':','//']\n",
    "sw_set = set(sw_list)\n",
    "\n",
    "# tokenization\n",
    "def process_data(string):\n",
    "    tokens = nltk.word_tokenize(string) # tokenization\n",
    "    punctuation_removed = [token.lower() for token in tokens if token.lower() not in sw_set]\n",
    "    punctuation_removed = [token.lower() for token in tokens ]\n",
    "    return punctuation_removed\n",
    "\n",
    "# create a function stemming() and loop through each word in a review\n",
    "def stemming(string):\n",
    "    # Stemming\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_string=[]\n",
    "    for w in string:\n",
    "        stemmed_string.append(ps.stem(w))\n",
    "    return stemmed_string\n",
    "\n",
    "\n",
    "# create a function  and loop through each word in  a review\n",
    "def lemmatization(string):\n",
    "    # import libraries\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list=[]\n",
    "    for word in string:\n",
    "        lemma_word=lemmatizer.lemmatize(word,pos='v') \n",
    "        lemma_list.append(lemma_word)\n",
    "    return lemma_list\n",
    "\n",
    "# Conbime all functions above and obtian cleaned text data \n",
    "def data_preprocessing(text_data):\n",
    "    ##tokenization, stop words removal, punctuation marks removel\n",
    "    processed_string=list(map(process_data,text_data))\n",
    "    #stemming\n",
    "    # stemming_string=list(map(stemming,processed_string)) #Data out prurals \n",
    "    ## lemmatization\n",
    "    lemma_string=list(map(lemmatization,processed_string))\n",
    "    \n",
    "    return lemma_string\n",
    "\n",
    "cleaned_data = data_preprocessing(df['CONTENT'])\n",
    "merged_data=[]\n",
    "for i in range(len(cleaned_data )):\n",
    "    merged_data.append(\" \".join(cleaned_data [i]))\n",
    "merged_df=pd.DataFrame(merged_data)\n",
    "merged_df.to_csv(\"merged_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:14:44.427317Z",
     "start_time": "2021-11-02T02:14:41.498874Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# obtener los ngrams  \n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vec = CountVectorizer(ngram_range=(4, 6)).fit(corpus) # minimo y maximo de palabras \n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def bigram_dictionary(data,n=None):\n",
    "    # n es la cantidad de n_grams que se desean adquirir \n",
    "    words_freq=get_top_n_bigram(data,n)\n",
    "    \n",
    "    dictionary={}\n",
    "    for i in range(len(words_freq)):\n",
    "        dictionary[words_freq[i][0]]=words_freq[i][1]\n",
    "    return dictionary\n",
    "\n",
    "BOW = bigram_dictionary(merged_data,15)# get top (merged_data,n= )\n",
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:14:53.100286Z",
     "start_time": "2021-11-02T02:14:50.804905Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Nos quedamos con el BAG que tengan un umbral de similitud de 0.3 (no se parezcan ) \n",
    "# y con la frase más larga  \n",
    "sentences = []\n",
    "BOW_new = {}\n",
    "for key0,value0 in BOW.items():\n",
    "    same = [ key for key,value in BOW.items() if value0==value]\n",
    "    max_same = max(same)\n",
    "    same_similarity = [s for s in same if nlp(s).similarity(nlp(max_same))<=0.4] + [max_same] #0.45 Umbral de similitud \n",
    "    sentences.append(same_similarity)\n",
    "    frases = []\n",
    "    for s in range(len(sentences)):\n",
    "        lista = list(map(len,sentences[s]))\n",
    "        try:\n",
    "            maxi = max(lista)\n",
    "            frases.append(sentences[s][lista.index(maxi)])\n",
    "            frases.append(sentences[s][-1])\n",
    "        except:\n",
    "            continue \n",
    "            \n",
    "for i in list(dict.fromkeys(frases)):\n",
    "    BOW_new[i] = BOW[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:15:03.962188Z",
     "start_time": "2021-11-02T02:14:53.103908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# twitter_mask = np.array(Image.open('./twitter_mask.png'))\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=1500,\n",
    "                  height=800,\n",
    "                  colormap='tab10',\n",
    "                  max_words=60,\n",
    "                  prefer_horizontal=1.0\n",
    "                 )\n",
    "                  \n",
    "plt.figure(figsize=(10,10), facecolor='w')\n",
    "Wcloud = cloud.generate_from_frequencies(BOW_new, max_font_size=800)\n",
    "plt.gca().imshow(Wcloud)\n",
    "plt.gca().set_title('Principales narrativas', fontdict=dict(size=16))\n",
    "plt.gca().axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
